{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9839619",
   "metadata": {},
   "source": [
    "Prepare MORO results for robustness analysis\n",
    "\n",
    "In this notebook, the following tasks are conducted:\n",
    "\n",
    "- Merge the results of the different seed runs\n",
    "- Create a first promising policy set by saving all pareto optimal solutions, for both the mean and 90th percentile based MORO results\n",
    "- Remove duplicate solutions from this set\n",
    "- Further reduce the number of policies in the pareto efficient sets with the use of k-means clustering\n",
    "- Combine the remaining optimal solutions from the mean and 90th percentile based sets into one promising policy set\n",
    "- Test these policies in a large set of experiments, which are sampled from the models uncertainty space\n",
    "- Save these experiments and outcomes for robustness analysis and scenario discovery\n",
    "\n",
    "NB: In this notebook, absolute path references/imports are used due to problems with relative paths in the authors Python interpreter environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base directory = C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\n",
      "No tmp/ folder for mean seed 0 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_mean_seed_0\\tmp\n",
      "No tmp/ folder for mean seed 1 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_mean_seed_1\\tmp\n",
      "Using archives_mean_seed_2\\tmp\\results_seed_2.csv\n",
      "Using archives_mean_seed_3\\tmp\\results_seed_3.csv\n",
      "No tmp/ folder for mean seed 4 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_mean_seed_4\\tmp\n",
      "No tmp/ folder for p90 seed 0 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_90_seed_0\\tmp\n",
      "Using archives_90_seed_1\\tmp\\42004.csv\n",
      "No tmp/ folder for p90 seed 2 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_90_seed_2\\tmp\n",
      "No tmp/ folder for p90 seed 3 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_90_seed_3\\tmp\n",
      "Using archives_90_seed_4\\tmp\\results_seed_4.csv\n",
      "Combined shape: (6286, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_RfR 0</th>\n",
       "      <th>0_RfR 1</th>\n",
       "      <th>0_RfR 2</th>\n",
       "      <th>1_RfR 0</th>\n",
       "      <th>1_RfR 1</th>\n",
       "      <th>1_RfR 2</th>\n",
       "      <th>2_RfR 0</th>\n",
       "      <th>2_RfR 1</th>\n",
       "      <th>2_RfR 2</th>\n",
       "      <th>3_RfR 0</th>\n",
       "      <th>...</th>\n",
       "      <th>A.5_DikeIncrease 1</th>\n",
       "      <th>A.5_DikeIncrease 2</th>\n",
       "      <th>Gelderland Expected Number of Deaths</th>\n",
       "      <th>Overijssel Expected Annual Damage</th>\n",
       "      <th>Overijssel Dike Investment Costs</th>\n",
       "      <th>Overijssel Expected Number of Deaths</th>\n",
       "      <th>RfR Total Costs</th>\n",
       "      <th>Expected Evacuation Costs</th>\n",
       "      <th>moro_kind</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>1.709083e+05</td>\n",
       "      <td>7.442279e+07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.129600e+09</td>\n",
       "      <td>519.302374</td>\n",
       "      <td>mean</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>1.932565e+05</td>\n",
       "      <td>1.121632e+08</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2.424000e+08</td>\n",
       "      <td>13526.675536</td>\n",
       "      <td>mean</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038923</td>\n",
       "      <td>2.121356e+05</td>\n",
       "      <td>9.611556e+07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>2.424000e+08</td>\n",
       "      <td>11483.530112</td>\n",
       "      <td>mean</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456134</td>\n",
       "      <td>1.557884e+06</td>\n",
       "      <td>9.014178e+07</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>3.070000e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>mean</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.652514</td>\n",
       "      <td>1.209853e+07</td>\n",
       "      <td>4.596357e+07</td>\n",
       "      <td>0.007557</td>\n",
       "      <td>1.212000e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>mean</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0_RfR 0  0_RfR 1  0_RfR 2  1_RfR 0  1_RfR 1  1_RfR 2  2_RfR 0  2_RfR 1  \\\n",
       "0        1        1        1        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        1        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   2_RfR 2  3_RfR 0  ...  A.5_DikeIncrease 1  A.5_DikeIncrease 2  \\\n",
       "0        0        1  ...                   0                   0   \n",
       "1        0        1  ...                   4                   3   \n",
       "2        0        1  ...                   7                   0   \n",
       "3        0        0  ...                   0                   0   \n",
       "4        0        1  ...                   4                   0   \n",
       "\n",
       "   Gelderland Expected Number of Deaths  Overijssel Expected Annual Damage  \\\n",
       "0                              0.001850                       1.709083e+05   \n",
       "1                              0.039648                       1.932565e+05   \n",
       "2                              0.038923                       2.121356e+05   \n",
       "3                              0.456134                       1.557884e+06   \n",
       "4                              0.652514                       1.209853e+07   \n",
       "\n",
       "   Overijssel Dike Investment Costs  Overijssel Expected Number of Deaths  \\\n",
       "0                      7.442279e+07                              0.000014   \n",
       "1                      1.121632e+08                              0.000013   \n",
       "2                      9.611556e+07                              0.000015   \n",
       "3                      9.014178e+07                              0.000960   \n",
       "4                      4.596357e+07                              0.007557   \n",
       "\n",
       "   RfR Total Costs  Expected Evacuation Costs  moro_kind  seed  \n",
       "0     1.129600e+09                 519.302374       mean     2  \n",
       "1     2.424000e+08               13526.675536       mean     2  \n",
       "2     2.424000e+08               11483.530112       mean     2  \n",
       "3     3.070000e+07                   0.000000       mean     2  \n",
       "4     1.212000e+08                   0.000000       mean     2  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# LOAD THE CSV RESULTS FROM ALL SEEDS & BOTH MOROs\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "SEEDS = range(5)                     # possible seeds for MORO runs (0 to 4))\n",
    "MORO_DIR_MAP = {                     # folder names on disk\n",
    "    \"mean\": \"archives_mean_seed_{seed}\",\n",
    "    \"p90\" : \"archives_90_seed_{seed}\",\n",
    "}\n",
    "\n",
    "# Base directory = folder where THIS notebook / script lives\n",
    "# BASE_DIR = Path(__file__).resolve().parent # When usning Jupyter files\n",
    "\n",
    "BASE_DIR = Path().resolve().parent # for Jupyter Notebooks\n",
    "print(f\" Base directory = {BASE_DIR}\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for kind, dir_pattern in MORO_DIR_MAP.items():\n",
    "    for seed in SEEDS:\n",
    "        # Absolute path to the archive directory\n",
    "        archive_dir = (BASE_DIR / dir_pattern.format(seed=seed)).resolve()\n",
    "        tmp_dir     = archive_dir / \"tmp\"\n",
    "\n",
    "        if not tmp_dir.exists():\n",
    "            print(f\"No tmp/ folder for {kind} seed {seed} → {tmp_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Grab *.csv files and pick the one with the highest NFE number (eg the last generation)\n",
    "        csv_files = sorted(tmp_dir.glob(\"*.csv\"), key=lambda p: int(re.findall(r\"\\d+\", p.stem)[0]))\n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files in {tmp_dir}\")\n",
    "            continue\n",
    "\n",
    "        final_csv = csv_files[-1]           # highest generation (last in sorted list)\n",
    "        print(f\"Using {final_csv.relative_to(BASE_DIR)}\")\n",
    "\n",
    "        df = pd.read_csv(final_csv)\n",
    "\n",
    "        # Tag bookkeeping columns\n",
    "        df[\"moro_kind\"] = kind\n",
    "        df[\"seed\"]      = seed\n",
    "        records.append(df)\n",
    "\n",
    "\n",
    "# COMBINE EVERYTHING\n",
    "if records:\n",
    "    df_moro = pd.concat(records, ignore_index=True)\n",
    "    print(\"Combined shape:\", df_moro.shape)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No MORO CSV files were found ─ check paths above.\")\n",
    "\n",
    "df_moro.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269aab91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216a588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tmp/ folder for mean seed 0 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_mean_seed_0\\tmp\n",
      "No tmp/ folder for mean seed 1 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_mean_seed_1\\tmp\n",
      "Using archives_mean_seed_2\\tmp\\results_seed_2.csv (mean seed 2)\n",
      "Using archives_mean_seed_3\\tmp\\results_seed_3.csv (mean seed 3)\n",
      "No tmp/ folder for mean seed 4 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_mean_seed_4\\tmp\n",
      "🟦 Mean‑based merged shape: (2828, 38)\n"
     ]
    }
   ],
   "source": [
    "# MERGE THE LAST‑GENERATION CSVs FOR MEAN‑BASED MORO RUNS\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "SEEDS = range(5)                                       # 0–4, the possible seeds for MORO runs\n",
    "BASE_DIR = Path().resolve().parent                     # adjust if needed\n",
    "records_mean = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    archive_dir = (BASE_DIR / f\"archives_mean_seed_{seed}\").resolve()\n",
    "    tmp_dir     = archive_dir / \"tmp\"\n",
    "\n",
    "    if not tmp_dir.exists():\n",
    "        print(f\"No tmp/ folder for mean seed {seed} → {tmp_dir}\")\n",
    "        continue\n",
    "\n",
    "    csv_files = sorted(\n",
    "        tmp_dir.glob(\"*.csv\"),\n",
    "        key=lambda p: int(re.findall(r\"\\d+\", p.stem)[0])\n",
    "    )\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files in {tmp_dir}\")\n",
    "        continue\n",
    "\n",
    "    final_csv = csv_files[-1]  # highest NFE\n",
    "    print(f\"Using {final_csv.relative_to(BASE_DIR)} (mean seed {seed})\")\n",
    "\n",
    "    df = pd.read_csv(final_csv)\n",
    "    df[\"seed\"] = seed\n",
    "    records_mean.append(df)\n",
    "\n",
    "# Combine and save\n",
    "if records_mean:\n",
    "    df_mean = pd.concat(records_mean, ignore_index=True)\n",
    "    print(\"🟦 Mean‑based merged shape:\", df_mean.shape)\n",
    "    df_mean.to_csv(\"mean_moro_merged.csv\", index=False)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No mean‑based MORO CSV files were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef74799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tmp/ folder for p90 seed 0 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_90_seed_0\\tmp\n",
      "Using archives_90_seed_1\\tmp\\42004.csv (p90 seed 1)\n",
      "No tmp/ folder for p90 seed 2 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_90_seed_2\\tmp\n",
      "No tmp/ folder for p90 seed 3 → C:\\Users\\tlwal\\OneDrive\\Documenten\\EPA\\Model Based Desision Making\\MBDM-main\\archives_90_seed_3\\tmp\n",
      "Using archives_90_seed_4\\tmp\\results_seed_4.csv (p90 seed 4)\n",
      "🟧 90ᵗʰ‑percentile merged shape: (3458, 38)\n"
     ]
    }
   ],
   "source": [
    "# MERGE THE LAST‑GENERATION CSVs FOR 90ᵗʰ‑PERCENTILE MORO RUNS\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "SEEDS = range(5)                                       # 0–4, the possible seeds for MORO runs\n",
    "BASE_DIR = Path().resolve().parent\n",
    "records_p90 = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    archive_dir = (BASE_DIR / f\"archives_90_seed_{seed}\").resolve()\n",
    "    tmp_dir     = archive_dir / \"tmp\"\n",
    "\n",
    "    if not tmp_dir.exists():\n",
    "        print(f\"No tmp/ folder for p90 seed {seed} → {tmp_dir}\")\n",
    "        continue\n",
    "\n",
    "    csv_files = sorted(\n",
    "        tmp_dir.glob(\"*.csv\"),\n",
    "        key=lambda p: int(re.findall(r\"\\d+\", p.stem)[0])\n",
    "    )\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files in {tmp_dir}\")\n",
    "        continue\n",
    "\n",
    "    final_csv = csv_files[-1]  # highest NFE\n",
    "    print(f\"Using {final_csv.relative_to(BASE_DIR)} (p90 seed {seed})\")\n",
    "\n",
    "    df = pd.read_csv(final_csv)\n",
    "    df[\"seed\"] = seed\n",
    "    records_p90.append(df)\n",
    "\n",
    "# Combine and save\n",
    "if records_p90:\n",
    "    df_p90 = pd.concat(records_p90, ignore_index=True)\n",
    "    print(\" 90ᵗʰ‑percentile merged shape:\", df_p90.shape)\n",
    "    df_p90.to_csv(\"p90_moro_merged.csv\", index=False)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No p90‑based MORO CSV files were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7976c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# General method Pareto filter\n",
    "\n",
    "OBJ_COLS = [\n",
    "    \"Gelderland Expected Number of Deaths\",\n",
    "    \"Overijssel Expected Annual Damage\",\n",
    "    \"Overijssel Dike Investment Costs\",\n",
    "    \"Overijssel Expected Number of Deaths\",\n",
    "    \"RfR Total Costs\",\n",
    "    \"Expected Evacuation Costs\",\n",
    "]\n",
    "\n",
    "def pareto_efficient(cost_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Boolean mask of Pareto‑efficient rows (minimise all objectives).\"\"\"\n",
    "    is_eff = np.ones(cost_matrix.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(cost_matrix):\n",
    "        if is_eff[i]:\n",
    "            is_eff[is_eff] = np.any(cost_matrix[is_eff] < c, axis=1)\n",
    "            is_eff[i] = True\n",
    "    return is_eff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b140c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟦 Unique Pareto (mean): 1770\n"
     ]
    }
   ],
   "source": [
    "# 1.  APPLY PARETO FILTER & DEDUPLICATION  (mean-based)\n",
    "\n",
    "costs_mean = df_mean[OBJ_COLS].values\n",
    "mask_mean  = pareto_efficient(costs_mean)\n",
    "df_pareto_mean = df_mean.loc[mask_mean].copy()\n",
    "\n",
    "# Infer lever columns: everything except objectives + meta colums\n",
    "NONLEVER = set(OBJ_COLS + [\"seed\"])          \n",
    "LEVER_COLS = [c for c in df_pareto_mean.columns if c not in NONLEVER]\n",
    "\n",
    "df_mean_unique = df_pareto_mean.drop_duplicates(subset=LEVER_COLS)\n",
    "print(f\"Unique Pareto (mean): {len(df_mean_unique)}\")\n",
    "\n",
    "df_mean_unique.to_csv(\"promising_policies_mean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Pareto (p90): 2399\n"
     ]
    }
   ],
   "source": [
    "# 2.  APPLY PARETO FILTER & DEDUPLICATION  (p90-based)\n",
    "\n",
    "costs_p90 = df_p90[OBJ_COLS].values\n",
    "mask_p90  = pareto_efficient(costs_p90)\n",
    "df_pareto_p90 = df_p90.loc[mask_p90].copy()\n",
    "\n",
    "NONLEVER = set(OBJ_COLS + [\"seed\"])          \n",
    "LEVER_COLS = [c for c in df_pareto_p90.columns if c not in NONLEVER]\n",
    "\n",
    "df_p90_unique = df_pareto_p90.drop_duplicates(subset=LEVER_COLS)\n",
    "print(f\"Unique Pareto (p90): {len(df_p90_unique)}\")\n",
    "\n",
    "df_p90_unique.to_csv(\"promising_policies_p90.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d669f",
   "metadata": {},
   "source": [
    "Both pareto efficient sets contain way too many policies to conduct an easily interpretable final robustness analysis (1770 and 2399 solutions of the mean and p90 based sets respectively). Therefore, it's needed to take small samples from both sets, while trying to save as much variety in the ultimate policy set as possible. To do so, we cluster the policies using k-means clustering. Hereby, the clusters are created in the outcome space and thus use the effects of policies as the measure of policy diversity. This provides the best guarantee to build understanding of what possible trade-offs are made when desiging policies. In order to end up with an easily interpretable and comprehensive set of 30 policies, we want to save 15 solutions from both the mean and p90 based sets. Therefore, we create 15 clusters.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028ae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged set size: 30 policies.\n"
     ]
    }
   ],
   "source": [
    "# K‑MEANS REDUCTION (15 clusters each) FOR MEAN & P90, THEN MERGE\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "OBJ_COLS = [\n",
    "    \"Gelderland Expected Number of Deaths\",\n",
    "    \"Overijssel Expected Annual Damage\",\n",
    "    \"Overijssel Dike Investment Costs\",\n",
    "    \"Overijssel Expected Number of Deaths\",\n",
    "    \"RfR Total Costs\",\n",
    "    \"Expected Evacuation Costs\",\n",
    "]\n",
    "\n",
    "def kmeans_reduce(df_in: pd.DataFrame, n_clusters: int = 15, label: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster on objective space and return the representative policy\n",
    "    closest to each cluster centre.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df_in[OBJ_COLS])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    df_in = df_in.copy()\n",
    "    df_in[\"cluster\"] = labels\n",
    "\n",
    "    closest_idx, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X)\n",
    "    df_red = df_in.iloc[closest_idx].copy()\n",
    "    df_red[\"source\"] = label  # tag origin (mean or p90)\n",
    "    return df_red\n",
    "\n",
    "# Reduce each set to 15 representatives \n",
    "df_mean15 = kmeans_reduce(df_mean_unique, n_clusters=15, label=\"mean\")\n",
    "df_p90_15 = kmeans_reduce(df_p90_unique,  n_clusters=15, label=\"p90\")\n",
    "\n",
    "# Merge the two reduced sets\n",
    "df_merged = pd.concat([df_mean15, df_p90_15], ignore_index=True)\n",
    "print(f\"Final merged set size: {len(df_merged)} policies.\")\n",
    "\n",
    "# Save results\n",
    "df_mean15.to_csv(\"representative_policies_mean_15.csv\", index=False)\n",
    "df_p90_15.to_csv(\"representative_policies_p90_15.csv\", index=False)\n",
    "df_merged.to_csv(\"representative_policies_combined_15.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188d6a9",
   "metadata": {},
   "source": [
    "As a next stape, we conduct experiments with the 30 remaining policies in a larger scenario set (N=1000), in order to be able to conduct a meaningfull robustness analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.1\n",
      "A.2\n",
      "A.3\n"
     ]
    }
   ],
   "source": [
    "#  TEST POLICIES UNDER 1000 NEW SCENARIOS\n",
    "from ema_workbench import (\n",
    "    Policy,                         \n",
    "    MultiprocessingEvaluator,\n",
    "    save_results,                   \n",
    ")\n",
    "from ema_workbench.em_framework import sample_uncertainties \n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "import numpy as np, random, os\n",
    "\n",
    "# Set up the model with the predefined problem formulation\n",
    "model, _ = get_model_for_problem_formulation(2)   # Again use PF 2\n",
    "lever_names = [l.name for l in model.levers]\n",
    "\n",
    "# Build Policy objects from the promising policies set\n",
    "policies = []\n",
    "for _, row in df_merged.iterrows():\n",
    "    lever_dict = {lv: row[lv] for lv in lever_names}\n",
    "    policies.append(Policy(f\"pol_{len(policies):03d}\", **lever_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198abece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 scenarios\n",
    "N_SCEN = 2 #100\n",
    "scenarios = sample_uncertainties(model, N_SCEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8043f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 60/60 [00:02<00:00, 26.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run experiments\n",
    "run_label = \"promising_resample_1000\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    experiments, outcomes = evaluator.perform_experiments(\n",
    "        scenarios=scenarios,\n",
    "        policies=policies,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d3ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: results/promising_resample_1000.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "\n",
    "save_path = f\"results/{run_label}.tar.gz\"\n",
    "save_results((experiments, outcomes), save_path)\n",
    "print(f\"✅ Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64882de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
